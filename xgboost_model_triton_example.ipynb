{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "191c3786-b58f-405e-9355-28868be93899",
   "metadata": {},
   "source": [
    "# Create a XGBoost Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d4d8713-7d6a-4197-b092-6f966a0da2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d036cc5-9ac5-4205-8df5-b9eaafd0b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data to perform binary classification\n",
    "seed = 7\n",
    "features = 9 # number of sample features\n",
    "samples = 10000 # number of samples\n",
    "X = numpy.random.rand(samples, features).astype('float32')\n",
    "Y = numpy.random.randint(2, size=samples)\n",
    "\n",
    "test_size = 0.33\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a0ba008-cac9-4c2a-ab62-c1e03bc96d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:54:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=16,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db2b6b3-adfc-46e3-b1e2-407e086467ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.64\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy: {:.2f}\".format(accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d9c88d-aacb-4c89-9083-b0fc45cfed5d",
   "metadata": {},
   "source": [
    "# Export, load and deploy XGBoost model in Triton Inference Server\n",
    "\n",
    "To deploy a trained model via Triton,  you need to set up a directory structure where you have a model repository directory, a model directory containing your model and a deployment configuration file.  \n",
    "\n",
    "The structure of the model repository should look like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d2c1f-79cb-43de-b361-89563954d4ba",
   "metadata": {},
   "source": [
    "<img src=\"triton_model_repository_layout.png\" width=400 height=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62807e-e8de-4022-894f-365a95a79549",
   "metadata": {},
   "source": [
    "### Save model to model location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6827b8d4-64be-40e3-9557-d0338c895dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('/home/ubuntu/model_repository/fil/1/xgboost.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ecb899-4dc7-4a2c-8658-edd36b4edb57",
   "metadata": {},
   "source": [
    "### Create and save config.pbtxt\n",
    "\n",
    "To deploy the model in Triton Inference Server, we need to create and save a protobuf config file called `config.pbtxt` under `/home/ubuntu/model_repository/fil/` directory that contains information about the model and the deployment. \n",
    "\n",
    "Triton server looks for this configuration file before deploying XGBoost model for inference. It'll setup the server parameters as per the configuration passed within `config.pbtxt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b63f2abd-c5fd-41bd-a6c2-2c8fb8490b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Writing config to file\n",
    "cat > /home/ubuntu/model_repository/fil/config.pbtxt <<EOL \n",
    "name: \"fil\"                              # Name of the model directory (fil in our case)\n",
    "backend: \"fil\"                           # Triton FIL backend for deploying forest models\n",
    "max_batch_size: 8192\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 9 ]                          # Input feature dimensions, in our sample case it's 9\n",
    "  }\n",
    "]\n",
    "output [\n",
    " {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]                          # Output 2 for binary classification model\n",
    "  }\n",
    "]\n",
    "instance_group [{ kind: KIND_GPU }]\n",
    "parameters [\n",
    "  {\n",
    "    key: \"model_type\"\n",
    "    value: { string_value: \"xgboost\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"predict_proba\"\n",
    "    value: { string_value: \"false\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"output_class\"\n",
    "    value: { string_value: \"true\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"threshold\"\n",
    "    value: { string_value: \"0.5\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"algo\"\n",
    "    value: { string_value: \"ALGO_AUTO\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"storage_type\"\n",
    "    value: { string_value: \"AUTO\" }\n",
    "  },\n",
    "  {\n",
    "    key: \"blocks_per_sm\"\n",
    "    value: { string_value: \"0\" }\n",
    "  }\n",
    "]\n",
    "\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7e360-4fb0-48b6-a148-f3881e94d1f4",
   "metadata": {},
   "source": [
    "### Inference via the Triton Client\n",
    "\n",
    "Test the inference by sending real inference request from Triton Client and checking the accuracy of responses.\n",
    "\n",
    "Start the Triton Server container:\\\n",
    "`$ sudo docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v $PWD/model_repository:/models nvcr.io/nvidia/tritonserver:22.01-py3 tritonserver --model-repository=/models`\n",
    "\n",
    "Check the status of the server connection by running the following curl command:\\\n",
    "`curl -v <IP of machine>:8000/v2/health/ready`\\\n",
    "which should return `HTTP/1.1 200 OK`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "589b7c4a-f4d3-4a1f-a7d8-eadc6eefbee7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 127.0.0.1:8000...\n",
      "* TCP_NODELAY set\n",
      "* Connected to localhost (127.0.0.1) port 8000 (#0)\n",
      "> GET /v2/health/ready HTTP/1.1\n",
      "> Host: localhost:8000\n",
      "> User-Agent: curl/7.68.0\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Length: 0\n",
      "< Content-Type: text/plain\n",
      "< \n",
      "* Connection #0 to host localhost left intact\n"
     ]
    }
   ],
   "source": [
    "! curl -v localhost:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871d4862-6d27-42ef-a5de-d3d88f4a5d8d",
   "metadata": {},
   "source": [
    "You can run the Triton Server client either as a container or embeded in your code. To run in your code, install the client via pip:\n",
    "\n",
    "`pip3 install tritonclient[http]`\n",
    "\n",
    "and import the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7ec3c7e-7134-41b2-8902-5d1de5be7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as triton_http"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef3ade2d-a49c-4008-be1c-e2879c4e1c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.79\n"
     ]
    }
   ],
   "source": [
    "# Set up HTTP client.\n",
    "http_client = triton_http.InferenceServerClient(\n",
    "    url='localhost:8000',\n",
    "    verbose=False,\n",
    "    concurrency=1\n",
    ")\n",
    "\n",
    "# Set up Triton input and output objects for both HTTP and GRPC\n",
    "triton_input_http = triton_http.InferInput(\n",
    "    'input__0',\n",
    "    (X_test.shape[0], X_test.shape[1]),\n",
    "    'FP32'\n",
    ")\n",
    "\n",
    "triton_input_http.set_data_from_numpy(X_test, binary_data=True)\n",
    "\n",
    "triton_output_http = triton_http.InferRequestedOutput(\n",
    "    'output__0',\n",
    "    binary_data=True\n",
    ")\n",
    "\n",
    "# Submit inference requests \n",
    "request_http = http_client.infer(\n",
    "    'fil',\n",
    "    model_version='1',\n",
    "    inputs=[triton_input_http],\n",
    "    outputs=[triton_output_http]\n",
    ")\n",
    "\n",
    "# Get results as numpy arrays\n",
    "result_http = request_http.as_numpy('output__0')\n",
    "\n",
    "# Check that we got the same accuracy as previously\n",
    "accuracy = accuracy_score(y_test, result_http)\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54152566-1b7f-49f2-a7f9-ffb3c4a75868",
   "metadata": {},
   "source": [
    "The above test accuracy score of the model deployed in Triton using FIL backend approximately matches with the one previously computed using XGBoost library's predict function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
